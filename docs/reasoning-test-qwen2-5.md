# モデルテスト結果とモデル選定ガイド

## 目次
1. [テスト環境](#テスト環境)
2. [qwen2.5:14b テスト結果](#qwen2514b-テスト結果)
3. [重要な発見](#重要な発見)
4. [モデル選定ガイド](#モデル選定ガイド)
5. [次のステップ](#次のステップ)

---

## テスト環境

**ハードウェア:**
- MacBook Air M1
- メモリ: 16GB
- ストレージ: SSD

**ソフトウェア:**
- ollama最新版
- macOS

**テスト日時:** 2025-11-08

---

## qwen2.5:14b テスト結果

### モデル基本情報

```
名前: Qwen2.5 14B Instruct
アーキテクチャ: qwen2
パラメータ数: 14.77B
ファイルサイズ: 8.37 GiB
量子化: Q4_K - Medium (4.87 BPW)
コンテキスト長: 32,768トークン
ライセンス: Apache 2.0
開発元: Alibaba Cloud (Qwen)
```

### テスト1: 簡単な計算

**プロンプト:**
```
1+1は？
```

**結果:**
```
1+1は2です。
```

**パフォーマンス:**
- モデルロード時間: 11.64秒（初回）
- 推論時間: 約3秒
- 合計実行時間: 14.79秒

### テスト2: 複雑な設計タスク（Reasoning）

**プロンプト:**
```
マイクロサービスでECサイトを設計する場合、どのようなサービス分割が適切ですか？
3つのサービスに分けて提案してください。
```

**結果:**
以下の3つのサービスを提案：
1. 商品情報管理サービス
2. ユーザー管理および注文処理サービス
3. 配送および在庫管理サービス

各サービスの詳細な説明とAPI連携についても言及。

**パフォーマンス:**
- モデルロード時間: 4.41秒（2回目、キャッシュ効果）
- 推論時間: 約70秒
- 合計実行時間: 74秒（1分14秒）

**メモリ使用量:**
```
Metal (GPU): 8,148 MB
CPU: 418 MB
KV Cache: 768 MB
合計: 約9.3 GB
```

**GPU活用:**
- 全49レイヤーをGPUにオフロード（M1 Metal使用）
- GPU加速により高速推論を実現

---

## 重要な発見

### ❌ qwen2.5:14bには思考プロセスがない

**期待していたこと:**
- `<think>...</think>` タグで思考プロセスを表示
- ステップバイステップの推論過程を確認

**実際の動作:**
- **直接回答を出力**
- 思考プロセスのタグは使用しない
- 内部的に考えているが、その過程は見えない

**例（期待 vs 実際）:**

**期待（Reasoning特化モデルの場合）:**
```
<think>
まず、ECサイトに必要な機能を整理する必要がある。
主要な機能は：商品管理、ユーザー管理、注文処理、配送...

次に、これらをどのように分割するか考える。
関連性の高い機能をグループ化すると...
</think>

最終回答：
以下の3つのサービスを提案します...
```

**実際（qwen2.5:14bの出力）:**
```
EC（電子商取引）サイトの設計においては...
以下の3つのマイクロサービスをお勧めします：

1. 商品情報管理サービス
2. ユーザー管理および注文処理サービス
3. 配送および在庫管理サービス
```

### ✅ qwen2.5:14bの強み

- **高品質な回答**（内容は実用的で詳細）
- **安定した動作**（16GBメモリで問題なし）
- **適度な速度**（70秒で複雑な設計提案）
- **GPU活用**（M1 Metalで高速化）

### ⚠️ Llamuneサービスへの影響

**Reasoningモードの要件:**
- 思考プロセスの可視化が**必須機能**
- ユーザーが「どう考えたか」を比較したい
- qwen2.5:14bでは**この要件を満たせない**

**結論:**
qwen2.5:14bは**コード生成モード**に適しているが、**Reasoningモード**には不向き。

---

## モデル選定ガイド

### Reasoningモード向けモデル

Reasoningモードでは、思考プロセスを明示的に出力するモデルが必要です。

**推奨モデル:**

| モデル | サイズ | 思考プロセス | メモリ要件 | 特徴 |
|--------|--------|------------|----------|------|
| **deepseek-r1:7b** | 約4GB | ✅ あり | 16GB可 | Reasoning特化、軽量 |
| **deepseek-r1:14b** | 約8GB | ✅ あり | 16GB可 | より高度な推論 |
| **qwq:32b** | 19GB | ✅ あり | 32GB必須 | 最高品質の推論 |

**特徴:**
- `<think>...</think>` タグで思考プロセスを出力
- 段階的な推論ステップが可視化される
- 「なぜその結論に至ったか」が分かる

### コード生成モード向けモデル

コード生成では、思考プロセスよりも**正確で実用的なコード**が重要です。

**推奨モデル:**

| モデル | サイズ | 用途 | メモリ要件 |
|--------|--------|------|----------|
| **qwen2.5-coder:7b** | 約4GB | コード生成全般 | 16GB可 |
| **deepseek-coder:6.7b** | 約4GB | コード生成特化 | 16GB可 |
| **qwen2.5:14b** | 8.4GB | 汎用（コード含む） | 16GB可 |
| **deepseek-coder:33b** | 19GB | 高品質コード生成 | 32GB必須 |

### メモリ別推奨構成

**16GB環境（現在）:**

**Reasoningモード:**
```bash
ollama pull deepseek-r1:7b    # 軽量、思考プロセスあり
ollama pull qwen2.5:14b       # 参考用（思考プロセスなし）
```

**コード生成モード:**
```bash
ollama pull qwen2.5-coder:7b
ollama pull deepseek-coder:6.7b
ollama pull qwen2.5:14b
```

**32GB環境（到着後）:**

**Reasoningモード:**
```bash
ollama pull deepseek-r1:14b
ollama pull qwq:32b          # 最高品質
ollama pull qwen2.5:32b
```

**コード生成モード:**
```bash
ollama pull deepseek-coder:33b
ollama pull qwen2.5-coder:32b
ollama pull codellama:34b
```

---

## 次のステップ

### 1. deepseek-r1:7bのテスト（進行中）

```bash
ollama pull deepseek-r1:7b
```

**確認事項:**
- 思考プロセスの出力形式
- タグの種類（`<think>`, `<reasoning>`, その他）
- 推論の深さと質
- パフォーマンス測定

**テストプロンプト（同じものを使用）:**
```
マイクロサービスでECサイトを設計する場合、どのようなサービス分割が適切ですか？
3つのサービスに分けて提案してください。
```

### 2. モデル比較の実施

同じプロンプトで以下を比較：
- qwen2.5:14b（思考プロセスなし）
- deepseek-r1:7b（思考プロセスあり）

**比較ポイント:**
- 回答の質
- 推論プロセスの可視性
- 実行時間
- メモリ使用量

### 3. Llamuneサービス設計の更新

**判明したこと:**
- Reasoningモードには専用モデルが必要
- qwen2.5:14bはコード生成モード向き
- UI設計で「思考プロセス」表示が重要

**更新が必要な設計:**
- モデル選択UIでモデルの特性を表示
- Reasoningモード時は思考プロセス対応モデルのみ選択可能
- コード生成モードは汎用モデルも選択可能

### 4. 32GB MacBook Air到着後

**大規模モデルのテスト:**
- qwq:32b - 最高品質のReasoning
- llama3.3:70b - 大規模汎用モデル
- deepseek-coder:33b - 高品質コード生成

**Llamuneサービスの本格開発:**
- Next.jsフロントエンド実装
- バッチ実行システム構築
- 結果比較UI開発

---

## 参考情報

### モデルのダウンロード元

- **Qwen2.5シリーズ**: https://huggingface.co/Qwen
- **DeepSeekシリーズ**: https://huggingface.co/deepseek-ai
- **QwQ**: https://huggingface.co/Qwen

### 関連ドキュメント

- [Ollama操作マニュアル](./ollama-operations.md)
- Llamune GitHub: https://github.com/unrcom/llamune

---

## まとめ

### 今日の主な成果

1. ✅ qwen2.5:14bの動作確認（16GB環境で安定動作）
2. ✅ Reasoningモードには専用モデルが必要と判明
3. ✅ パフォーマンス測定データ取得
4. ✅ モデル選定基準の確立
5. 🔄 deepseek-r1:7bのテスト準備中

### 重要な教訓

**「汎用LLM ≠ Reasoning特化モデル」**

- 汎用モデル（qwen2.5:14b）: 高品質な回答は出せるが、思考プロセスは見えない
- Reasoning特化モデル（deepseek-r1, qwq）: 思考過程を明示的に出力

Llamuneサービスの「Reasoningモード」を実現するには、**Reasoning特化モデルが必須**。

---

**最終更新:** 2025-11-08 19:30
**次回更新予定:** deepseek-r1:7bテスト完了後

