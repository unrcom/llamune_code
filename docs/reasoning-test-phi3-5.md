# Phi-3.5 Reasoning テスト結果

## 目次
1. [モデル基本情報](#モデル基本情報)
2. [テスト結果](#テスト結果)
3. [重要な発見](#重要な発見)
4. [他モデルとの比較](#他モデルとの比較)
5. [Llamuneサービスでの推奨用途](#llamuneサービスでの推奨用途)
6. [まとめ](#まとめ)

---

## モデル基本情報

**モデル名:** Phi 3.5 Mini Instruct  
**アーキテクチャ:** phi3  
**パラメータ数:** 3.82B  
**ファイルサイズ:** 2.03 GiB  
**量子化:** Q4_0 (4.55 BPW)  
**コンテキスト長:** 131,072トークン（128K）  
**ライセンス:** MIT（商用利用可）  
**開発元:** Microsoft  
**特徴:** Reasoning dense dataで訓練、数学・論理推論に強い

---

## テスト環境

**ハードウェア:**
- MacBook Air M1
- メモリ: 16GB
- GPU: Apple M1 Metal

**ソフトウェア:**
- ollama最新版
- macOS

**テスト日時:** 2025-11-08

---

## テスト結果

### テスト: 複雑な設計タスク（Reasoning）

**プロンプト:**
```
マイクロサービスでECサイトを設計する場合、どのようなサービス分割が適切ですか？
3つのサービスに分けて提案してください。
```

**出力（要約）:**
```
マイクロサービスアーキテクチャを用いてECサイトを設計する場合、
以下の3つの基本的なマイクロサービス分割例を提案：

1. 商品（Product Service）
   - 責任: 商品リポジトリにアクセス、商品の登録/更新
   - 機能: 在庫検索、購入機能、商品・カテゴリ情報管理
   - インターフェース: RESTful API

2. ユーザー（User Service）
   - 責任: ユーザーデータ管理、ログイン/認証
   - 機能: アカウント情報保護、権限管理
   - インターフェース: セキュリティ確保されたAPI

3. チェックアウトおよび支払情報（Checkout and Payment Service）
   - 責任: カート作成、支払処理、金銭関連データ管理
   - 機能: チェックアウトプロセス、支払オペレーション
   - インターフェース: 静的API、メタデータ保存

さらに、必要に応じて以下のサービスも提案：
4. レビュー（Review Service）
5. 配送情報（Shipping Service）
6. セキュリティおよびログ（Security and Log Service）
```

**パフォーマンス:**
- モデルロード時間: 0.58秒（**全モデル中最速**）
- 推論時間: 約90秒
- 合計実行時間: 91秒（1分31秒）

**メモリ使用量:**
```
Metal (GPU): 2.0 GB
KV Cache: 1.5 GB
CPU: 52 MB
合計: 約4.3 GB
```

**GPU最適化:**
- 全33レイヤーをM1 GPUにオフロード
- Metal活用で高速推論

---

## 重要な発見

### ✅ Phi-3.5の強み

1. **最速のロード時間**
   - 0.58秒で起動
   - deepseek-r1:7b（0.88s）より速い
   - qwen2.5:14b（4.41s）より大幅に速い

2. **超軽量**
   - ファイルサイズ: 2.03GB（全モデル中最小）
   - メモリ使用: 4.3GB
   - 16GB環境で余裕をもって動作

3. **128Kコンテキスト長**
   - 超長文ドキュメント処理が可能
   - 長いコード、詳細な仕様書の処理に適している
   - 他のモデル（8K-32K）より大幅に長い

4. **詳細で実用的な回答**
   - 3つのサービスに加え、追加の3つも提案
   - RESTful APIの具体例を含む
   - セキュリティやログまで言及
   - 実装に直結する内容

5. **完全な日本語対応**
   - 中国語混在なし
   - deepseek-r1:7bと違って扱いやすい

6. **Reasoning能力**
   - "reasoning dense data"で訓練されている
   - 数学と論理推論に強い設計

### ❌ 制限事項

1. **思考プロセスの出力がない**
   - `Thinking...` のような表示なし
   - 直接回答のみ
   - qwen2.5:14b、gemma2:9bと同じパターン
   - **Reasoningモードには不向き**

2. **若干の誤字・不自然な表現**
   - "レpository" → "リポジトリ"
   - "オペレーシione" → "オペレーション"
   - "ネットワールド" → "ネットワーク"
   - "スケーリフティング" → "スケーリング"
   - おそらく多言語トークナイザの影響

3. **やや冗長**
   - 3つの提案を求めたが6つ提案
   - 簡潔さよりも網羅性を優先

---

## 他モデルとの比較

### パフォーマンス比較

| 項目 | phi3.5 | deepseek-r1:7b | qwen2.5:14b | gemma2:9b |
|------|--------|----------------|-------------|-----------|
| **ファイルサイズ** | 2.03GB | 4.36GB | 8.37GB | 5.06GB |
| **ロード時間** | **0.58s** ⭐ | 0.88s | 4.41s | 1.37s |
| **実行時間** | 91s | 78s | 70s | **52s** ⭐ |
| **メモリ使用** | **4.3GB** ⭐ | 5GB | 9GB | 7.5GB |
| **コンテキスト** | **128K** ⭐ | 131K | 32K | 8K |
| **思考プロセス** | ❌ | ✅ | ❌ | ❌ |
| **回答品質** | ✅ 高 | ⚠️ 不安定 | ✅ 高 | ✅ 最高 |
| **構造化** | ○ | △ | ○ | ◎ |
| **誤字** | ⚠️ あり | なし | なし | なし |
| **言語** | 日本語 | 中国語混在 | 日本語 | 日本語 |

### 特徴別比較

**軽量性:** phi3.5 > deepseek-r1:7b > gemma2:9b > qwen2.5:14b  
**起動速度:** phi3.5 > deepseek-r1:7b > gemma2:9b > qwen2.5:14b  
**推論速度:** gemma2:9b > qwen2.5:14b > deepseek-r1:7b > phi3.5  
**コンテキスト:** phi3.5 (128K) > deepseek-r1:7b (131K) > qwen2.5:14b (32K) > gemma2:9b (8K)  
**構造化:** gemma2:9b > qwen2.5:14b = phi3.5 > deepseek-r1:7b

---

## Llamuneサービスでの推奨用途

### ✅ 推奨される用途

**1. コード生成モード**
- 128Kコンテキストを活かした長いコード生成
- 複数ファイルにまたがるプロジェクト全体の理解
- 詳細な仕様書からのコード生成
- リファクタリング（大規模コードベース）

**2. 長文ドキュメント処理**
- 技術文書の要約
- 長い仕様書の分析
- APIドキュメントの解析
- コードレビュー（大規模）

**3. 数学・論理タスク**
- アルゴリズム設計
- データ構造の最適化
- 複雑な計算ロジック
- パフォーマンス最適化提案

**4. リソース制約環境**
- メモリが限られた環境（4-8GB程度）
- 高速起動が必要な場面
- バッチ処理で複数モデルを順次実行

### ❌ 推奨されない用途

**Reasoningモード**
- 思考プロセスの可視化が不可能
- DeepSeek-R1系を使用すべき

**高精度な日本語生成**
- 誤字が散見される
- gemma2:9bやqwen2.5:14bの方が安定

---

## Llamuneサービス設計への影響

### モデル選択UI

**コード生成モードでの表示:**
```
[ ] phi3.5 (推奨)
    特徴: 超軽量・高速起動・128Kコンテキスト
    用途: 長いコード生成、複数ファイル処理
    
[ ] qwen2.5:14b
    特徴: バランス型・高品質
    用途: 汎用コード生成
    
[ ] gemma2:9b
    特徴: 高品質・構造化出力
    用途: ドキュメント生成
```

### 特殊機能の提案

**128Kコンテキスト活用機能:**
- プロジェクト全体をコンテキストに含める
- 複数ファイルの同時処理
- 長い履歴を保持した会話

**実装例:**
```javascript
// 長いコンテキストを活かした機能
const projectContext = `
  // file1.js の内容 (5000行)
  // file2.js の内容 (3000行)
  // file3.js の内容 (4000行)
  // ... 合計で50,000トークン程度
`;

// phi3.5なら余裕で処理可能
const response = await generateCode(
  model: 'phi3.5',
  context: projectContext,
  prompt: 'このプロジェクト全体をリファクタリングしてください'
);
```

---

## 次のステップ

### 追加テスト候補

**数学的推論（phi3.5の強み）:**
```bash
ollama run phi3.5 "3人の海賊が金貨100枚を分配する。最年長の海賊が提案し、過半数の賛成で決定。反対多数なら最年長は海に落とされ、次の海賊が提案。最年長の海賊の最適戦略は？"
```

**長文コード生成（128Kコンテキスト活用）:**
```bash
ollama run phi3.5 "React、TypeScript、Tailwind CSSを使って、ユーザー管理機能を持つ完全なダッシュボードアプリを作成してください。認証、CRUD操作、レスポンシブデザインを含めてください。"
```

**複数ファイル処理:**
```bash
# 実際のプロジェクトファイルを複数読み込んでリファクタリング提案
```

### 32GB環境での比較テスト

**phi3.5 vs より大きなモデル:**
- phi3.5 (3.8B) vs phi-4 (14B)
- 軽量だが高性能なphi3.5の優位性確認
- 128Kコンテキストの実用性検証

---

## まとめ

### Phi-3.5の評価

**✅ 優れている点:**
- **最軽量かつ最速起動**（2.03GB、0.58秒）
- **128Kコンテキスト**（超長文処理可能）
- **16GB環境で余裕**（4.3GBのみ使用）
- **数学・論理推論に強い**
- 完全な日本語対応

**⚠️ 注意点:**
- 思考プロセスなし（Reasoningモード不可）
- 若干の誤字あり
- やや冗長な出力

**🎯 最適な用途:**
- コード生成モード（特に長いコード）
- 数学・アルゴリズムタスク
- リソース制約環境
- 長文ドキュメント処理

### Llamuneサービスでの位置づけ

**推奨シナリオ:**
1. **コード生成モードのデフォルト**（軽量・高速）
2. **長文処理専用モデル**（128Kコンテキスト）
3. **バッチ処理の先頭**（高速起動）

**非推奨シナリオ:**
- Reasoningモード（思考プロセスなし）
- 超高精度な日本語生成（誤字リスク）

### プロジェクトの現状

**テスト完了モデル:**
- ✅ deepseek-r1:7b（Reasoning特化）
- ✅ qwen2.5:14b（汎用・バランス）
- ✅ phi3.5（軽量・128Kコンテキスト）
- ✅ gemma2:9b（高品質・構造化）

**次のマイルストーン:**
- 🔜 32GB MacBook Air到着（3日後）
- 🔜 大規模Reasoningモデルテスト（qwq:32b、deepseek-r1:14b）
- 🔜 Llamune MVP実装開始

---

## 参考情報

### Phi-3.5について

**公式情報:**
- Microsoft Blog: https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/
- Ollama Library: https://ollama.com/library/phi3.5

**特徴:**
- Small Language Model (SLM) の代表格
- "Textbooks Are All You Need" アプローチ
- 高品質データで訓練
- ラズベリーパイでも動作

### 関連ドキュメント

- [Ollama操作マニュアル](./ollama-operations.md)
- [Qwen2.5 Reasoningテスト](./reasoning-test-qwen2-5.md)
- [DeepSeek-R1:7B Reasoningテスト](./reasoning-test-deepseek-r1-7b.md)
- [Gemma2:9B Reasoningテスト](./reasoning-test-gemma2-9b.md)
- Llamune GitHub: https://github.com/unrcom/llamune

---

**最終更新:** 2025-11-08 22:30  
**テスト実施者:** mop  
**次回更新予定:** 32GB環境でのテスト完了後

